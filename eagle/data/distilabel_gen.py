# -*- coding: utf-8 -*-
import json
import os
import glob
import argparse
from pathlib import Path
from typing import List, Dict, Any, Optional
from tqdm.auto import tqdm

try:
    from distilabel.pipeline import Pipeline
    from distilabel.steps import LoadDataFromDicts
    from distilabel.steps.tasks import TextGeneration
    from distilabel.llms import OpenAILLM
    from datasets import Dataset, concatenate_datasets
    import pyarrow.parquet as pq
except ImportError as e:
    print(f"å¯¼å…¥é”™è¯¯: {e}")
    print("è¯·å…ˆå®‰è£…ä¾èµ–é¡¹: pip install -r requirements.txt")
    exit(1)


def load_ultra_dataset(directory_path: str) -> List[Dict]:
    """åŠ è½½Ultraæ•°æ®é›†"""
    parquet_files = glob.glob(os.path.join(directory_path, "*.parquet"))
    result_list = []
    
    for file_path in parquet_files:
        table = pq.read_table(file_path)
        records = table.to_pydict()
        num_rows = len(next(iter(records.values())))
        row_list = [
            {key: records[key][i] for key in records} 
            for i in range(num_rows)
        ]
        result_list.extend(row_list)
    
    return result_list


def load_sharegpt_dataset(dataset_path: str) -> List[Dict]:
    """åŠ è½½ShareGPTæ•°æ®é›†"""
    data = []
    if dataset_path.endswith('.jsonl'):
        with open(dataset_path, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    data.append(json.loads(line))
    elif dataset_path.endswith('.json'):
        with open(dataset_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {dataset_path}")
    
    return data


def prepare_conversation_data(dataset_path: str, dataset_type: str = "sharegpt", max_examples: Optional[int] = None) -> List[Dict]:
    """å‡†å¤‡å¯¹è¯æ•°æ®ï¼Œè½¬æ¢ä¸º distilabel å¯ä»¥å¤„ç†çš„æ ¼å¼"""
    # åŠ è½½æ•°æ®é›†
    if dataset_type == "ultra":
        dataset = load_ultra_dataset(dataset_path)
    else:
        dataset = load_sharegpt_dataset(dataset_path)
    
    # å¦‚æœæŒ‡å®šäº†æœ€å¤§ç¤ºä¾‹æ•°ï¼Œåˆ™é™åˆ¶æ•°æ®é‡
    if max_examples is not None and max_examples > 0:
        original_length = len(dataset)
        dataset = dataset[:max_examples]
        print(f"è°ƒè¯•æ¨¡å¼ï¼šä½¿ç”¨å‰ {max_examples} æ¡æ•°æ®ï¼ˆåŸå§‹æ•°æ®é‡: {original_length}ï¼‰")
    
    # å‡†å¤‡è¾“å‡ºæ•°æ® - ä¸ºæ¯ä¸ªåŠ©æ‰‹å›åˆåˆ›å»ºä¸€ä¸ªç”Ÿæˆä»»åŠ¡
    output_data = []
    
    for idx, conversation in enumerate(dataset):
        conversations_list = conversation.get("conversations", [])
        
        # å¤„ç†ä¸åŒçš„æ•°æ®æ ¼å¼
        if not conversations_list:
            if "items" in conversation:
                conversations_list = conversation["items"]
            else:
                conversations_list = conversation.get("messages", [])
        
        conversation_id = conversation.get("id", f"conversation_{idx}")
        
        # ä¸ºæ¯ä¸ªåŠ©æ‰‹å›åˆåˆ›å»ºç”Ÿæˆä»»åŠ¡
        context = ""
        assistant_turn_count = 0
        
        for i, msg in enumerate(conversations_list):
            role = msg.get("from") or msg.get("role")
            content = msg.get("value") or msg.get("content")
            
            if role in ["human", "user"]:
                context += f"Human: {content}\n\n"
            elif role in ["gpt", "assistant"]:
                # ä¸ºå½“å‰åŠ©æ‰‹å›åˆåˆ›å»ºç”Ÿæˆä»»åŠ¡
                current_context = context + "Assistant:"
                
                output_data.append({
                    "conversation_context": current_context,
                    "conversation_id": conversation_id,
                    "original_conversations": conversations_list,
                    "assistant_turn_index": assistant_turn_count,
                    "original_response": content  # ä¿å­˜åŸå§‹å›ç­”ç”¨äºæ¯”è¾ƒ
                })
                
                # æš‚æ—¶ç”¨å ä½ç¬¦æ›´æ–°ä¸Šä¸‹æ–‡ï¼ˆåç»­ä¼šè¢«æ›¿æ¢ï¼‰
                context += f"Assistant: [PLACEHOLDER_{assistant_turn_count}]\n\n"
                assistant_turn_count += 1
    
    return output_data


def reconstruct_conversations(processed_data: List[Dict]) -> List[Dict]:
    """é‡å»ºå®Œæ•´çš„å¯¹è¯"""
    # æŒ‰ conversation_id åˆ†ç»„ç”Ÿæˆçš„å›ç­”
    responses_by_conversation = {}
    
    for item in processed_data:
        conv_id = item["conversation_id"]
        if conv_id not in responses_by_conversation:
            responses_by_conversation[conv_id] = {
                "original_conversations": item["original_conversations"],
                "responses": {}
            }
        
        turn_index = item["assistant_turn_index"]
        generated_response = item["generation"]  # distilabel ç”Ÿæˆçš„å›ç­”
        responses_by_conversation[conv_id]["responses"][turn_index] = generated_response
    
    # é‡å»ºå¯¹è¯
    output_data = []
    
    for conv_id, conv_data in responses_by_conversation.items():
        original_conversations = conv_data["original_conversations"]
        generated_responses = conv_data["responses"]
        
        # é‡å»ºå¯¹è¯
        new_conversations = []
        assistant_turn_count = 0
        
        for msg in original_conversations:
            role = msg.get("from") or msg.get("role")
            content = msg.get("value") or msg.get("content")
            
            if role in ["human", "user"]:
                new_conversations.append({
                    "from": "human",
                    "value": content
                })
            elif role in ["gpt", "assistant"]:
                # ä½¿ç”¨ç”Ÿæˆçš„å›ç­”æ›¿æ¢åŸå§‹å›ç­”
                new_content = generated_responses.get(assistant_turn_count, content)
                new_conversations.append({
                    "from": "gpt",
                    "value": new_content
                })
                assistant_turn_count += 1
        
        output_data.append({
            "id": conv_id,
            "conversations": new_conversations
        })
    
    return output_data


def main():
    parser = argparse.ArgumentParser(description='ä½¿ç”¨distilabelé‡æ–°ç”ŸæˆShareGPTæ•°æ®é›†çš„å›ç­”')
    parser.add_argument('--dataset', type=str, required=True, help='ShareGPTæ•°æ®é›†æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--vllm_url', type=str, required=True, help='vLLMæœåŠ¡çš„APIåœ°å€')
    parser.add_argument('--output_dir', type=str, default='./regenerated_data', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--dataset_type', type=str, default='sharegpt', 
                        choices=['sharegpt', 'ultra'], help='æ•°æ®é›†ç±»å‹')
    parser.add_argument('--temperature', type=float, default=0.7, help='ç”Ÿæˆæ–‡æœ¬çš„æ¸©åº¦å‚æ•°')
    parser.add_argument('--max_new_tokens', type=int, default=2048, help='æ¯æ¬¡ç”Ÿæˆçš„æœ€å¤§tokenæ•°')
    parser.add_argument('--model_name', type=str, default='Qwen/Qwen2.5-72B-Instruct', 
                        help='æ¨¡å‹åç§°')
    parser.add_argument('--batch_size', type=int, default=4, help='æ‰¹å¤„ç†å¤§å°')
    parser.add_argument('--use_cache', action='store_true', help='æ˜¯å¦ä½¿ç”¨ç¼“å­˜')
    parser.add_argument('--timeout', type=float, default=120.0, help='APIè¯·æ±‚è¶…æ—¶æ—¶é—´(ç§’)')
    parser.add_argument('--max_retries', type=int, default=3, help='æœ€å¤§é‡è¯•æ¬¡æ•°')
    parser.add_argument('--max_examples', type=int, default=None, help='ç”¨äºè°ƒè¯•ï¼šä»…ä½¿ç”¨å‰Næ¡æ•°æ®')
    
    args = parser.parse_args()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output_dir, exist_ok=True)
    
    # å‡†å¤‡æ•°æ®
    print(f"å¼€å§‹åŠ è½½æ•°æ®é›†: {args.dataset}")
    input_data = prepare_conversation_data(args.dataset, args.dataset_type, args.max_examples)
    print(f"å‡†å¤‡äº† {len(input_data)} ä¸ªç”Ÿæˆä»»åŠ¡")
    
    # å¦‚æœæ˜¯è°ƒè¯•æ¨¡å¼ï¼Œæä¾›ä¸€äº›é¢å¤–ä¿¡æ¯
    if args.max_examples is not None:
        print(f"ğŸ”§ è°ƒè¯•æ¨¡å¼ï¼šä»…å¤„ç†å‰ {args.max_examples} æ¡å¯¹è¯æ•°æ®")
        if len(input_data) > 0:
            print(f"ğŸ“Š ç¤ºä¾‹æ•°æ®é¢„è§ˆï¼š")
            sample_data = input_data[0]
            print(f"  - å¯¹è¯ID: {sample_data.get('conversation_id', 'N/A')}")
            print(f"  - åŠ©æ‰‹å›åˆç´¢å¼•: {sample_data.get('assistant_turn_index', 'N/A')}")
            print(f"  - ä¸Šä¸‹æ–‡é•¿åº¦: {len(sample_data.get('conversation_context', ''))}")
            print(f"  - åŸå§‹å›ç­”é•¿åº¦: {len(sample_data.get('original_response', ''))}")
            print(f"  - ä¸Šä¸‹æ–‡é¢„è§ˆ: {sample_data.get('conversation_context', '')[:100]}...")
        else:
            print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å¯¹è¯æ•°æ®")
    
    # åˆ›å»ºæµæ°´çº¿
    with Pipeline(name="sharegpt-regeneration") as pipeline:
        # ç¬¬ä¸€æ­¥ï¼šåŠ è½½æ•°æ®
        load_data = LoadDataFromDicts(
            name="load_data",
            data=input_data,
            batch_size=args.batch_size
        )
        
        # ç¬¬äºŒæ­¥ï¼šåˆå§‹åŒ–LLM
        llm = OpenAILLM(
            base_url=args.vllm_url,
            api_key="sk-none",  # type: ignore
            model=args.model_name,
            timeout=args.timeout,
            max_retries=args.max_retries,
        )
        
        # ç¬¬ä¸‰æ­¥ï¼šé‡æ–°ç”ŸæˆåŠ©æ‰‹å›ç­”
        response_generator = TextGeneration(
            name="response_generator",
            llm=llm,
            input_batch_size=args.batch_size,
            system_prompt=(
                "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ã€‚è¯·æ ¹æ®å¯¹è¯ä¸Šä¸‹æ–‡ç”Ÿæˆåˆé€‚çš„å›ç­”ã€‚"
                "å›ç­”åº”è¯¥è‡ªç„¶ã€æœ‰å¸®åŠ©ï¼Œå¹¶ä¸å¯¹è¯çš„è¯­æ°”å’Œé£æ ¼ä¿æŒä¸€è‡´ã€‚"
            ),
            template="{{ conversation_context }}",
            columns=["conversation_context"]
        )
        
        # è¿æ¥æ­¥éª¤
        load_data >> response_generator
    
    # è¿è¡Œæµæ°´çº¿
    print(f"å¼€å§‹å¤„ç†æ•°æ®é›†")
    print(f"ä½¿ç”¨æ¨¡å‹: {args.model_name}")
    print(f"vLLM URL: {args.vllm_url}")
    print(f"æ‰¹å¤„ç†å¤§å°: {args.batch_size}")
    print(f"æ¸©åº¦: {args.temperature}")
    print(f"æœ€å¤§æ–°ç”Ÿæˆtokens: {args.max_new_tokens}")
    if args.max_examples is not None:
        print(f"è°ƒè¯•æ¨¡å¼ï¼šæœ€å¤§ç¤ºä¾‹æ•°: {args.max_examples}")
    
    # æµ‹è¯• API è¿æ¥
    try:
        import requests
        test_url = f"{args.vllm_url.rstrip('/')}/models"
        print(f"æµ‹è¯• API è¿æ¥: {test_url}")
        response = requests.get(test_url, timeout=10)
        print(f"API è¿æ¥æµ‹è¯•ç»“æœ: {response.status_code}")
        if response.status_code == 200:
            print("API è¿æ¥æ­£å¸¸")
        else:
            print(f"API è¿æ¥å¼‚å¸¸: {response.text}")
    except Exception as e:
        print(f"API è¿æ¥æµ‹è¯•å¤±è´¥: {e}")
        print("è¯·æ£€æŸ¥ vLLM æœåŠ¡æ˜¯å¦æ­£å¸¸è¿è¡Œ")
    
    try:
        distiset = pipeline.run(
            use_cache=args.use_cache,
            parameters={
                "response_generator": {
                    "llm": {
                        "generation_kwargs": {
                            "temperature": args.temperature,
                            "max_new_tokens": args.max_new_tokens,
                            "top_p": 0.9,
                        }
                    }
                }
            }
        )
    except Exception as e:
        print(f"è¿è¡Œæµæ°´çº¿æ—¶å‡ºé”™: {e}")
        print("å¯èƒ½çš„åŸå› :")
        print("1. vLLM æœåŠ¡æœªå¯åŠ¨æˆ–é…ç½®é”™è¯¯")
        print("2. æ¨¡å‹åç§°ä¸æ­£ç¡®")
        print("3. API ç«¯ç‚¹è·¯å¾„ä¸æ­£ç¡®")
        print("4. ç½‘ç»œè¿æ¥é—®é¢˜")
        raise
    
    # ä¿å­˜ç»“æœ
    output_path = Path(args.output_dir)
    
    # ä¿å­˜ä¸ºArrowæ ¼å¼
    distiset.save_to_disk(
        distiset_path=output_path / "distiset",
        max_shard_size="500MB",
        num_proc=4,
        save_card=True,
        save_pipeline_config=True,
        save_pipeline_log=True
    )
    
    # è·å–ç”Ÿæˆçš„æ•°æ®
    config_name = "default" if "default" in distiset else next(iter(distiset.keys()))
    dataset = distiset[config_name]
    
    # å¤„ç†DatasetDictç»“æ„
    if hasattr(dataset, 'keys'):
        combined_dataset = concatenate_datasets([dataset[split] for split in dataset.keys()])
    else:
        combined_dataset = dataset
    
    # è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼ä»¥ä¾¿å¤„ç†
    processed_data = []
    for item in combined_dataset:
        processed_data.append(item)
    
    # é‡å»ºå®Œæ•´å¯¹è¯
    print("é‡å»ºå®Œæ•´å¯¹è¯...")
    sharegpt_data = reconstruct_conversations(processed_data)
    
    # ä¿å­˜ä¸ºJSON
    json_path = output_path / "regenerated_complete.json"
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(sharegpt_data, f, ensure_ascii=False, indent=2)
    
    # ä¿å­˜ä¸ºJSONL
    jsonl_path = output_path / "regenerated_complete.jsonl"
    with open(jsonl_path, 'w', encoding='utf-8') as f:
        for item in sharegpt_data:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')
    
    print(f"å¤„ç†å®Œæˆï¼")
    print(f"æ€»å…±å¤„ç†äº† {len(sharegpt_data)} æ¡å¯¹è¯")
    print(f"ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
    print(f"JSONæ ¼å¼: {json_path}")
    print(f"JSONLæ ¼å¼: {jsonl_path}")
    
    # æ˜¾ç¤ºç¤ºä¾‹
    if sharegpt_data:
        print("\nç¤ºä¾‹æ•°æ®ï¼š")
        print(json.dumps(sharegpt_data[0], ensure_ascii=False, indent=2))


if __name__ == "__main__":
    main() 

# ä½¿ç”¨ç¤ºä¾‹:
# python distilabel_gen.py \
#   --dataset /path/to/dataset.jsonl \
#   --vllm_url http://localhost:8000/v1 \
#   --model_name Qwen/Qwen2.5-72B-Instruct \
#   --batch_size 4 \
#   --temperature 0.7 \
#   --max_new_tokens 2048 \
#   --output_dir ./output \
#   --use_cache
#
# è°ƒè¯•æ¨¡å¼ç¤ºä¾‹ (åªä½¿ç”¨å‰5æ¡æ•°æ®):
# python distilabel_gen.py \
#   --dataset /path/to/dataset.jsonl \
#   --vllm_url http://localhost:8000/v1 \
#   --model_name Qwen/Qwen2.5-72B-Instruct \
#   --batch_size 2 \
#   --max_examples 5 \
#   --temperature 0.7 \
#   --output_dir ./debug_output